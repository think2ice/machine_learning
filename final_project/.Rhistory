rf$test$confusion
1 - sum(diag(rf$test$confusion) / sum(rf$test$confusion))
rf
1 - sum(diag(rf$test$confusion) / sum(rf$test$confusion))
rf$test$confusion
diag(rf$test$confusion)
dim(rf$test$confusion)
1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
set.seed(777)
# 0. Reading the dataset
# Locally
setwd("/Users/manel/Documents/Universidad/MIRI/Q1B/ML/my_labs/machine_learning.git/final_project")
yeast <- read.csv2(file = "yeast.csv", header = FALSE, dec = ".")
# Directly from UCI machine learning repository
# yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
#                         yeast/yeast.data"),header = FALSE)
# 1. Basic analysis
# Adding row and column names to the dataset
cols.names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc","class")
names(yeast) <- cols.names
# The first column should be treated as the row.names but there are inputs that are repeated
# To deal with this problem, the function make names is ideal
row.names(yeast) <- make.names(yeast$seq.name, unique = TRUE)
yeast <- yeast[,-1]
# Basic inspection of the dataset
dim(yeast)
summary(yeast)
str(yeast)
# the erl variable appears as numerical but in fact it is a binary variable, so it
# should be redefined as a factor
yeast$erl <- as.factor(yeast$erl)
# Plots of 2 vs 2 variables
plot(yeast)
# Plots of the histogram for each continuous variable
par(mfrow = c(3,3))
for (i in c(1:4,6:8)) {
hist(yeast[,i], main = names(yeast)[i])
}
# Barplot of the erl variable
barplot(table(yeast$erl), main = colnames(yeast)[5])
# Barplot of the target, class
barplot(table(yeast$class), main = colnames(yeast)[9])
# 3. Preprocess the data
# Notice that our target variable, class, is completely unbalanced
# With CYT, NUC, MIT and ME3 the most frequent classes
table(yeast[,ncol(yeast)])
# Looking at the data density, perhaps a transformation could be applied to
# variables erl and pox
summary(yeast[,6:7])
# pox is always almost 0
# 4. Define training and test data
# Get the training and test data
N <- dim(yeast)[1]
learn <- sample(1:N, round(2/3*N))
yeast.train <- yeast[learn,]
yeast.test <- yeast[-learn,]
dim(yeast.test)
yeast[row.names(yeast[-learn,]),]
match(row.names(yeast),row.names(yeast.test))
library(TunePareto)
# Prepare a crossvalidation 10x10 method to get the best model with
# several classifiers
k <- 10
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
# 5. Classification methods
# baseline: the error that we get predicting always the most probable class
(baseline <- 100*(1 - max(table(yeast$class))/nrow(yeast)))
# let's see if this can be improved using:
# 1. Naive Bayes
# 2. LDA
# 3. QDA
# 4. KNN
# 5. PCA + Neural Networks
# 6. Random Forest
# With cross-validation
# prepare the structure to store the partial results
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
yeast.nb <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(yeast.nb,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict(yeast.nb,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
cv.results
# Average of the validation error
(VA.error <- mean(cv.results[,"VA error"]))
Model.CV <- function (k, method)
{
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA and QDA")
}
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
mean(cv.results[,"VA error"])
}
Model.CV(10,"NaiveBayes")
library(e1071)
Model.CV(10,"NaiveBayes")
Model.CV(10,"LDA")
library(MASS)
Model.CV(10,"LDA")
Model.CV(10,"QDA")
?lda
?qda
predict(rf)$class
rf
predict(rf)$class
predict(rf)
Model.CV(10,"RandomForest")
Model.CV <- function (k, method)
{
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = va[,-9],
ytest = va[,9])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
mean(cv.results[,"VA error"])
}
Model.CV(10,"RandomForest")
summary(yeast)
dim(yeast)
Model.CV <- function (k, method)
{
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = va,
ytest = va)
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
mean(cv.results[,"VA error"])
}
Model.CV(10,"RandomForest")
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
summary(va)
summary(va)
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
va <- unlist(CV.folds[[1]][[1]])
summary(va)
va
Model.CV <- function (k, method)
{
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-9],
ytest = yeast.train[va,-9])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
mean(cv.results[,"VA error"])
}
Model.CV(10,"RandomForest")
Model.CV <- function (k, method)
{
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-9],
ytest = yeast.train[va,9])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
mean(cv.results[,"VA error"])
}
Model.CV(10,"RandomForest")
Model.CV(10,"NaiveBayes")
Model.CV(10,"LDA")
Model.CV(10,"QDA")
k <- 10
k <- 10
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-9],
ytest = yeast.train[va,9])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
mean(cv.results[,"VA error"])
}
Model.CV("NaiveBayes")
Model.CV("LDA")
Model.CV("QDA")
Model.CV("RandomForest")
rf <- randomForest(formula = class ~., data = yeast.train, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
rf.test.error <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
print(rf)
which(colnames(yeast)==class)
which(colnames(yeast)=='class')
yeast.nb <- naiveBayes (yeast.train$class ~ ., data=yeast.train, laplace=3)
# predict the left-out data
pred <- predict(yeast.nb,newdata=yeast.test)
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:8) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
rf <- randomForest(formula = class ~., data = yeast.train, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
print(rf)
