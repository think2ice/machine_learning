else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(cv.results, mean(cv.results[,"VA error"]))
# mean(cv.results[,"VA error"])
}
# Test the behavior for Naive Bayes
Model.CV("NaiveBayes")
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(c(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
# Test the behavior for Naive Bayes
Model.CV("NaiveBayes")
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(cv.results, mean(cv.results[,"VA error"]))
# mean(cv.results[,"VA error"])
}
# Test the behavior for Naive Bayes
[a,b] <- Model.CV("NaiveBayes")
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(cv.results, mean(cv.results[,"VA error"]))
# mean(cv.results[,"VA error"])
}
[a,b] <- Model.CV("NaiveBayes")
c(a,b) <- Model.CV("NaiveBayes")
?list
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(list(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
# Test the behavior for Naive Bayes
Model.CV("NaiveBayes")
# Test the behavior for LDA
Model.CV("LDA")
Model.CV("QDA")
# Test the behavior for Random Forest
Model.CV("RandomForest")
model <- qda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
# predict TR data
pred <- predict(model)$class
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:8) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
model <- qda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
?qda
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- qda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(list(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
Model.CV("QDA")
model <- lda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
# predict TR data
pred <- predict(model)$class
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:8) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
model <- lda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
# predict TR data
pred <- predict(model)$class
(tt <- table(Truth=yeast.test$class, Predicted=pred))
pred <- predict(model, newdata = yeast.test)
(tt <- table(Truth=yeast.test$class, Predicted=pred))
model <- lda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
# predict TR data
pred <- predict(model, newdata = yeast.test)
(tt <- table(Truth=yeast.test$class, Predicted=pred))
pred <- predict(model, newdata = yeast.test)$class
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:8) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
(tt <- table(Truth=yeast.test$class, Predicted=pred))
summary(yeast$class)
length(levels(yeast$class))
yeast.nb <- naiveBayes (yeast.train$class ~ ., data=yeast.train, laplace=3)
# predict the test data
pred <- predict(yeast.nb,newdata=yeast.test)
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:length(levels(yeast$class))) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
model <- lda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
# predict Test data
pred <- predict(model, newdata = yeast.test)$class
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:length(levels(yeast$class))) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
baseline
?qda
model <- qda(yeast.train ~ . , data = yeast.train, CV=FALSE)
model <- qda(yeast.train$class ~ . , data = yeast.train, CV=FALSE)
model <- qda(yeast.train$class ~ . , data = yeast.train)
yeast.nb <- naiveBayes (yeast.train$class ~ ., data=yeast.train, laplace=3)
# predict the test data
pred <- predict(yeast.nb,newdata=yeast.test)
(tt <- table(Truth=yeast.test$class, Predicted=pred))
(error <- 100*(1-sum(diag(tt))/sum(tt)))
# Reduction of the error
100*(baseline-error)/baseline
# Real error for each class
for (i in 1:length(levels(yeast$class))) {
cat(levels(yeast.test$class)[i])
print(1- tt[i,i]/sum(tt[i,]))
}
summary(yeast$class)
droplevels(yeast.test)
dim(yeast.data)
dim(yeast.test)
dim(droplevels(yeast.test))
?droplevels
?qda
library(nnet)
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=2, maxit=200, decay=0.01)
summary(model.nnet)
dim(yeast)
summary(model.nnet)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/nlearn))
dim(yeast.train)[1]
dim(yeast.train)[2]
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=2, maxit=200, decay=0.01)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=yeast.test, type="class"))
t2 <- table(p2,yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=20, maxit=200, decay=0.01)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=yeast.test, type="class"))
t2 <- table(p2,yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
p2
t2
t2
table(yeast.test$class)
table(yeast$class)
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=20, maxit=200)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=yeast.test, type="class"))
t2 <- table(p2,yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=30, maxit=200)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=yeast.test, type="class"))
t2 <- table(p2,yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
## For a specific model, in our case the neural network, the function train() in {caret} uses a "grid" of model parameters
## and trains using a given resampling method (in our case we will be using 10x10 CV). All combinations are evaluated, and
## the best one (according to 10x10 CV) is chosen and used to construct a final model, which is refit using the whole training set
## Thus train() returns the constructed model (exactly as a direct call to nnet() would)
## In order to find the best network architecture, we are going to explore two methods:
## a) Explore different numbers of hidden units in one hidden layer, with no regularization
## b) Fix a large number of hidden units in one hidden layer, and explore different regularization values (recommended)
## doing both (explore different numbers of hidden units AND regularization values) is usually a waste of computing resources (but notice that train() would admit it)
## Let's start with a)
## set desired sizes
library(caret)
install.packages('caret')
library(caret)
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
decays <- 10^seq(-3,0,by=0.1)
decays
model.10x10CV <- train (class ~., data = yeast, subset=learn, method='nnet', maxit = 500, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
decays <- 10^seq(-2,0,by=0.2)
decays
decays <- 10^seq(-3,0,by=0.2)
decays
decays <- 10^seq(-3,0,by=0.2)
model.10x10CV <- train (class ~., data = yeast, subset=learn, method='nnet', maxit = 200, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
warnings()
