model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- qda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(list(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
k <- 10
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
library(TunePareto)
# Prepare a crossvalidation 10x10 method to get the best model with
# several classifiers
k <- 10
CV.folds <- generateCVRuns(yeast.train$class, ntimes=1, nfold=k, stratified=TRUE)
# Test the behavior for Naive Bayes
Model.CV("NaiveBayes")
# Test the behavior for LDA
Model.CV("LDA")
# Test the behavior for QDA
Model.CV("QDA")
# Test the behavior for KNN
Model.CV("KNN")
# Test the behavior for Random Forest
Model.CV("RandomForest")
Model.CV("LDA")
# Test the behavior for QDA
rf <- randomForest(formula = class ~., data = yeast.train, subset = learn, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
print(rf)
rf <- randomForest(formula = class ~., data = yeast.train, subset = learn, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
rf <- randomForest(formula = class ~., data = yeast, subset = learn, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
print(rf)
rf <- randomForest(formula = class ~., data = yeast.train, xtest = yeast.test[,-9],
ytest = yeast.test[,9])
print(rf)
unlist(CV.folds[[1]][[j]])
unlist(CV.folds[[1]][[1]])
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- qda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
rf <- randomForest(formula = class ~., data = yeast.train, subset = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(list(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
Model.CV("RandomForest")
learn
learn[-va,]
Model.CV <- function (method)
{
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
resp.var <- which(colnames(yeast.train)=='class')
for (j in 1:k) {
# get VA data
va <- unlist(CV.folds[[1]][[j]])
tr <- yeast.train[-va,]
# train on TR data
if (method == "NaiveBayes") {
model <- naiveBayes(tr$class ~ . , data = tr, laplace=3)
# predict TR data
pred.tr <- predict(model,newdata=tr)
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "QDA"){
model <- qda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "LDA"){
model <- lda(tr$class ~ . , data = tr, CV=FALSE)
# predict TR data
pred.tr <- predict(model)$class
tab <- table(tr$class, pred.tr)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
pred.va <- predict(model,newdata=yeast.train[va,])$class
tab <- table(yeast.train[va,]$class, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
else if (method == "RandomForest"){
tr <- which(!is.na(match(row.names(yeast.train),row.names(tr))))
rf <- randomForest(formula = class ~., data = yeast.train, subset = tr, xtest = yeast.train[va,-resp.var],
ytest = yeast.train[va,resp.var])
cv.results[j,"TR error"] <- 1 - sum(diag(rf$confusion[,-11]) / sum(rf$confusion[,-11]))
cv.results[j,"VA error"] <- 1 - sum(diag(rf$test$confusion[,-11]) / sum(rf$test$confusion[,-11]))
cv.results[j,"fold"] <- j
}
else {
stop("Unknown method. The only valid methods ara NaiveBayes, LDA, QDA and RandomForest")
}
}
return(list(cv.results, mean(cv.results[,"VA error"])))
# mean(cv.results[,"VA error"])
}
Model.CV("RandomForest")
test <- which(!is.na(match(row.names(yeast),row.names(yeast.test))))
test
learn
sort(learn)
test
library("FactoMineR")
par(mfrow = c(1,2))
# find the index of the test individuals
test <- which(!is.na(match(row.names(yeast),row.names(yeast.test))))
# test <- as.vector(test)
pca.yeast <- PCA(yeast, quali.sup = c(5,9), ind.sup = test)
# Plot of the individuals (using class as a color)
par(mfrow = c(1,1))
plot(pca_yeast$ind$coord, col = yeast$class)
plot(pca.yeast$ind$coord, col = yeast$class)
plot(pca.yeast$ind$coord, col = yeast$class)
# Try to guess how many PC are the optimal for this problem
# Plot of the eigenvalues
plot(pca.yeast$eig$eigenvalue, type = "b", main = "Eigenvalues")
pca.yeast$eig
# Following the Kaiser rule (keeping at least 80% of the variance) we decided to
# keep 5 eigenvalues (which is the default number in PCA))
# Now we can train a neural network
pca.yeast.train <- pca.yeast$ind$coord
summary(yeast)
row.names(pca.yeast.train)
row.names(yeast.train)
sort(row.names(yeast.train))
row.names(pca.yeast.train)[1:10]
sort(row.names(yeast.train))[1:10]
pca.yeast.train <- pca.yeast$ind$coord
row.names(pca.yeast.train)[1:10]
sort(row.names(yeast.train))[1:10]
pca.yeast <- PCA(yeast, quali.sup = c(5,9), ind.sup = !test)
pca.yeast <- PCA(yeast, quali.sup = c(5,9), ind.sup = test)
# Plot of the individuals (using class as a color)
par(mfrow = c(1,1))
plot(pca.yeast$ind$coord, col = yeast$class)
# Try to guess how many PC are the optimal for this problem
# Plot of the eigenvalues
plot(pca.yeast$eig$eigenvalue, type = "b", main = "Eigenvalues")
pca.yeast$eig
?PCA
row.names(pca.yeast.train)[1:10]
sort(row.names(yeast.train))[1:10]
row.names(pca.yeast$ind.sup$coord)[1:10]
sort(row.names(pca.yeast.train))[1:10]
sort(row.names(pca.yeast.train))[1:10]
sort(row.names(yeast.train))[1:10]
sort(row.names(pca.yeast$ind.sup$coord))[1:10]
yeast.train[sort(row.names(pca.yeast.train))[1:10],]
pca.yeast.train[row.names(yeast.train),]
row.names(yeast.train)[1:5]
pca.yeast.train[row.names(yeast.train)[1:5],]
str(yeast.train)
which(colnames(yeast.train) == 'class')
pca.yeast.train <- pca.yeast$ind$coord
pca.yeast.train[row.names(yeast.train),] <- yeast.train[,which(colnames(yeast.train) == 'class')]
pca.yeast.test <- pca.yeast$ind.sup$coord
pca.yeast.test <- pca.yeast$ind.sup$coord
pca.yeast.train[row.names(yeast.test),] <- yeast.test[,which(colnames(yeast.train) == 'class')]
dim(pca.yeast.test)
dim(yeast.test)
dim(pca.yeast.train)
dim(yeast.train)
pca.yeast.train <- pca.yeast$ind$coord
dim(pca.yeast.train)
dim(yeast.train)
pca.yeast.train <- pca.yeast$ind$coord
pca.yeast.train[row.names(yeast.train),'class'] <- yeast.train[,which(colnames(yeast.train) == 'class')]
pca.yeast.train[,'class'] <- 'TEST'
pca.yeast.train <- pca.yeast$ind$coord
pca.yeast.train <- as.data.frame(pca.yeast.train)
pca.yeast.train[row.names(yeast.train),'class'] <- yeast.train[,which(colnames(yeast.train) == 'class')]
dim(pca.yeast.train)
pca.yeast.test <- pca.yeast$ind.sup$coord
pca.yeast.test <- as.data.frame(pca.yeast.test)
pca.yeast.train[row.names(yeast.test),'class'] <- yeast.test[,which(colnames(yeast.train) == 'class')]
rownames(yeast.test)[1:10]
row.names(pca.yeast.test)[1:10]
pca.yeast.test[rownames(yeast.test)[1:10], 'class']
pca.yeast.test[rownames(yeast.test)[1:10], 6]
pca.yeast.test[rownames(yeast.test)[1:10], ]
pca.yeast.train <- pca.yeast$ind$coord
pca.yeast.train <- as.data.frame(pca.yeast.train)
pca.yeast.train[row.names(yeast.train),'class'] <- yeast.train[,which(colnames(yeast.train) == 'class')]
pca.yeast.test <- pca.yeast$ind.sup$coord
pca.yeast.test <- as.data.frame(pca.yeast.test)
pca.yeast.test[row.names(yeast.test),'class'] <- yeast.test[,which(colnames(yeast.test) == 'class')]
pca.yeast.train <- pca.yeast$ind$coord
pca.yeast.train <- as.data.frame(pca.yeast.train)
pca.yeast.train[row.names(yeast.train),'class'] <- yeast.train[,which(colnames(yeast.train) == 'class')]
pca.yeast.test <- pca.yeast$ind.sup$coord
pca.yeast.test <- as.data.frame(pca.yeast.test)
pca.yeast.test[row.names(yeast.test),'class'] <- yeast.test[,which(colnames(yeast.test) == 'class')]
pca.yeast.test[rownames(yeast.test)[1:10], ]
yeast.test[1:10,]
?nnet
pca.yeast.data <- rbind(pca.yeast.train,pca.yeast.test)
1:dim(pca.yeast.train)[1]
# Now we can train a neural network
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=30, maxit=200, decay=0.01)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=yeast.test, type="class"))
t2 <- table(p2,yeast.test$class)
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.01)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.01)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
library(caret)
install.packages('caret')
# Now we can train a neural network
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=300, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=300, decay=0.001)
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=400, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=500, decay=0.001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.0001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
decays <- 10^seq(-3,0,by=0.1)
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=30, maxit=200, decay=0.0001)
summary(model.nnet)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
library(caret)
trc <- trainControl (method="repeatedcv", number=10, repeats=10)
decays <- 10^seq(-3,0,by=0.1)
model.10x10CV <- train (class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], method='nnet', maxit = 200, trace = FALSE,
tuneGrid = expand.grid(.size=30,.decay=decays), trControl=trc)
model.10x10CV$results
## and the best model found
model.10x10CV$bestTune
model.nnet <- nnet(class ~., data = yeast, subset=learn, size=30, maxit=200, decay=0.7943)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(yeast.train)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=30, maxit=200, decay=0.7943)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(yeast.test)[1]))
# Fit the best model with all the training and check the results:
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=30, maxit=200, decay=0.7943)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(pca.yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.07943)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(pca.yeast.test)[1]))
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.07943)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(pca.yeast.test)[1]))
table(yeast[,ncol(yeast)])
table(yeast[,which(colnames(yeast)=='class')])
table(yeast[,which(colnames(yeast)=='class')])
warning()
decays <- 10^seq(-3,0,by=0.1)
model.10x10CV <- train (class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], method='nnet', maxit = 200, trace = FALSE,
tuneGrid = expand.grid(.size=20,.decay=decays), trControl=trc)
model.10x10CV$results
## and the best model found
model.10x10CV$bestTune
# Fit the best model with all the training and check the results:
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=20, maxit=200, decay=0.5011872)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(pca.yeast.test)[1]))
model.10x10CV <- train (class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], method='nnet', maxit = 200, trace = FALSE,
tuneGrid = expand.grid(.size=10,.decay=decays), trControl=trc)
## We can inspect the full results
model.10x10CV$results
## and the best model found
model.10x10CV$bestTune
model.nnet <- nnet(class ~., data = pca.yeast.data, subset=1:dim(pca.yeast.train)[1], size=10, maxit=200, decay=0.398)
# TR error
p1 <- as.factor(predict (model.nnet, type="class"))
t1 <- table(p1,pca.yeast.train$class)
(error_rate.learn <- 100*(1-sum(diag(t1))/dim(pca.yeast.train)[1]))
# TEST error
p2 <- as.factor(predict (model.nnet, newdata=pca.yeast.test, type="class"))
t2 <- table(p2,pca.yeast.test$class)
(error_rate.test <- 100*(1-sum(diag(t2))/dim(pca.yeast.test)[1]))
