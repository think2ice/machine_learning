pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
## have a look at the results ...
cv.results
## Now we see the large variability across the different VA data
## What one really uses is the average of the last column
(VA.error <- mean(cv.results[,"VA error"]))
## You may think that instead the average is quite good, but this need not be the case,
## as we shall see in a minute
## Now let us change the number of folds/experiments k
## To do this, we embed the previous code into a function; we also prepare it for either
## LDA or QDA
## The function returns the AVERAGE CROSS-VALIDATION error
DA.CV <- function (k, method)
{
CV.folds <- generateCVRuns(data$target, ntimes=1, nfold=k, stratified=TRUE)
cv.results <- matrix (rep(0,4*k),nrow=k)
colnames (cv.results) <- c("k","fold","TR error","VA error")
cv.results[,"TR error"] <- 0
cv.results[,"VA error"] <- 0
cv.results[,"k"] <- k
priors <- c(prior.1,prior.2) # for clarity
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
if (method == "LDA") { my.da.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE) }
else if (method == "QDA") { my.da.TR <- qda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE) }
else stop("Wrong method")
# predict TR data
pred.va <- predict (my.da.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.da.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
mean(cv.results[,"VA error"])
}
## Armed with this function, we are going to produce a plot by changing k
the.Ks <- 2:20
res <- vector("numeric",length(the.Ks)+1)
for (k in the.Ks) res[k] <- DA.CV(k,"LDA")
## let us see the results
plot(res[-1],type="b",xlab="Value of k",ylab="average CV error", ylim=c(0.22,0.3))
## Now I'll reveal you the truth ...
# The following function computes the true probability of error for two-class normally distributed
# features with equal covariance matrices and arbitrary means and priors
# the formula is well-known and can be found in pattern recognition textbooks
prob.error <- function (Pw1, Pw2, Sigma, Mu1, Mu2)
{
# Numerically correct way for t(x) %*% solve(M) %*% (x), i.e., for x^T M^{-1} x
quad.form.inv <- function (M, x)
{
drop(crossprod(x, solve(M, x)))
}
stopifnot (Pw2+Pw1==1,Pw2>0,Pw1>0,Pw2<1,Pw1<1)
alpha <- log(Pw2/Pw1)
D <- quad.form.inv (Sigma, Mu1-Mu2)
A1 <- (alpha-D/2)/sqrt(D)
A2 <- (alpha+D/2)/sqrt(D)
Pw1*pnorm(A1)+Pw2*(1-pnorm(A2))
}
## In this case we get:
(pe <- prob.error (prior.1,prior.2,R[,,1],c(-1,0),c(1,1/2)))
## add it to the plot (it may be that if falls outside the plot, in the bottom)
abline(pe,0)
## We can see that no estimation gives you the correct result, most (if not all) of them over-estimate it
## (there could be some under-estimation too); and the thing stabilizes gently to a slight over-estimate as k increases
## So there is no "best" value for k to be used in k-CV; it depends largely on the amount of data
## more data allows one to decrease k; less and less data forces to use a large k
## Aha! but these numbers are random numbers, because they depend on the specific partition that
## k-CV uses; correct: so a good idea if your computational resources permit is to iterate the process ...
iters <- 20
res <- vector("numeric",length(the.Ks)+1)
for (k in the.Ks) res[k] <- mean(replicate(iters,DA.CV(k,"LDA")))
## let us see the results
plot(res[-1],type="b",xlab="Value of k", ylim=c(0.24,0.3))
# in blue the true error
abline(pe,0,col="blue")
# in red the training error
abline(lda.tr.error/100, 0, col="red")
# in green the LOOCV error (notice this one cannot be iterated)
lda.LOOCV <- lda(target ~ X1 + X2, data = data, prior=priors, CV=TRUE)
(ct <- table(data$target, lda.LOOCV$class))
(lda.LOOCV.error <- 1-sum(diag(prop.table(ct))))
abline(lda.LOOCV.error, 0, col="green")
legend("topright", legend=c("average k-CV error", "true error", "training error", "LOOCV error"),
pch=c(1,1), col=c("black", "blue","red","green"))
# in this case both training error and LOOCV errors coincide; in general the former will be larger
## In comparison to the previous results, we can see that now all estimations are over-estimates, and
## the thing has stabilized quite a lot; and the training error is an optimistic value
## Ask yourself why the CV error is consistently an over-estimate of the true error (that is, is a pessimistic value)
## Now we'll use these errors for model selection (you will see that this is possible, because they will have lower
## values for better models); however, you can realize that if, in addition to model selection, you want to give a more
## correct estimation of the true error of a model, the CV error is not correct. That is why you need a separate TEST sample
## There is no optimal k, but you have to choose one: very often this is a very imprecise function of data sample size
## and computational resources (larger values of k entail a heavier computational burden)
## Standard choices are 5CV or 10CV, 10x10 CV and LOOCV
## Let us choose 10x10 CV ... and use it to choose among LDA and QDA for this problem
## Yes ... we know LDA is the optimal, but this we don't know in practice
(lda.10x10.CV <- mean(replicate(10,DA.CV(10,"LDA"))))
(qda.10x10.CV <- mean(replicate(10,DA.CV(10,"QDA"))))
## It is a close shave but the result is correct; now we will follow the standard procedure:
## We need a test set to estimate the true error of our LDA model
## We make it rather large to have a more significant estimation
N.test <- 10000
N1 <- round(prior.1*N.test)
N2 <- N.test - N1
data1 <- mvrnorm(N1, mu=c(-1,0), Sigma=R[,,1])
data2 <- mvrnorm(N2, mu=c(1,1/2), Sigma=R[,,1])
## Now we create a dataframe with all data stacked by rows
data.test <- data.frame (rbind(data1,data2),target=as.factor(c(rep('1',N1),rep('2',N2))))
## First we refit our LDA model to the full sample used for learning
my.lda <- lda(target ~ X1 + X2, data = data)
## And now we make it predict the test set
(ct <- table(data.test$target, predict(my.lda, newdata=data.test)$class))
## total error (in percentage)
(pe.hat <- 1-sum(diag(prop.table(ct))))
## Now a 95% CI around it
dev <- sqrt(pe.hat*(1-pe.hat)/N.test)*1.967
sprintf("(%f,%f)", pe.hat-dev,pe.hat+dev)
## true error of our model was
pe
## so the method works
?generateCVRuns
for (j in 1:k)
{
# get VA data
va <- unlist(CV.folds[[1]][[j]])
# train on TR data
my.lda.TR <- lda(target ~ X1 + X2, data = data[-va,], prior=priors, CV=FALSE)
# predict TR data
pred.va <- predict (my.lda.TR)$class
tab <- table(data[-va,]$target, pred.va)
cv.results[j,"TR error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
# predict VA data
pred.va <- predict (my.lda.TR, newdata=data[va,])$class
tab <- table(data[va,]$target, pred.va)
cv.results[j,"VA error"] <- 1-sum(tab[row(tab)==col(tab)])/sum(tab)
cv.results[j,"fold"] <- j
}
## have a look at the results ...
cv.results
perc <- c(0.2, 0.3, 0.3, 0.2)
cost1 <- c(800, 1000, 1000, 1000)
cost2 <- c(1000, 800, 1000, 1000)
cost3 <- c(1000, 1000, 10, 1000)
cost4 <- c(1000,1000,1000, 40)
perc*cost1
sum(perc*cost1)
sum(perc*cost2)
sum(perc*cost3)
sum(perc*cost4)
10000/(512000/2)
10000/(512000)
10000/(512000/2)
10000/(512000/2)/5
10000/(512000/2)/5*3*80
10000/(512000/2)/5*3*80
10000/(512000/2)/5*3*80*80
100*2
99.84*2
perc <- c(.31, .25, .29, .15)
perc <- c(.31, .25, .29, .15)
cost1 <- c(2, 10000, 10000, 2)
cost2 <- c(10000, 2, 10000, 10000)
cost3 <- c(10000, 200, 200, 10000)
cost4 <- c(10000, 10000, 10000, 2)
function calculate_cost(percentage, cost){
(sum(percentage*cost))
}
perc <- c(.31, .25, .29, .15)
cost1 <- c(2, 10000, 10000, 2)
cost2 <- c(10000, 2, 10000, 10000)
cost3 <- c(10000, 200, 200, 10000)
cost4 <- c(10000, 10000, 10000, 2)
calculate_cost <- function(percentage, cost){
(sum(percentage*cost))
}
for (i in c(cost1, cost2, cost3, cost4)){
calculate_cost(perc, i)
}
for (i in c(cost1, cost2, cost3, cost4)){
(calculate_cost(perc, i))
}
for (i in c(cost1, cost2, cost3, cost4)){
(calculate_cost(perc, i))
}
calculate_cost(perc,cost1)
for (i in c(cost1, cost2, cost3, cost4)){
print(calculate_cost(perc, i))
}
costs <- list(cost1, cost2, cost3, cost4)
for (i in list){
calculate_cost(perc, i)
}
costs <- list(cost1, cost2, cost3, cost4)
for (i in costs){
calculate_cost(perc, i)
}
costs <- list(cost1, cost2, cost3, cost4)
for (i in costs){
(calculate_cost(perc, i))
}
costs <- list(cost1, cost2, cost3, cost4)
for (i in costs){
print(calculate_cost(perc, i))
}
perc <- c(.31, .25, .29, .15)
cost1 <- c(2, 10000, 10000, 2)
cost2 <- c(10000, 2, 10000, 2)
cost3 <- c(10000, 200, 200, 200)
cost4 <- c(10000, 10000, 10000, 2)
calculate_cost <- function(percentage, cost){
sum(percentage*cost)
}
costs <- list(cost1, cost2, cost3, cost4)
for (i in costs){
print(calculate_cost(perc, i))
}
"
CREATE MATERIALIZED VIEW v1
BUILD IMMEDIATE
REFRESH COMPLETE ON DEMAND
ENABLE QUERY REWRITE
AS SELECT A, SUM(D) FROM T GROUP BY A;
CREATE MATERIALIZED VIEW v2
BUILD IMMEDIATE
REFRESH COMPLETE ON DEMAND
ENABLE QUERY REWRITE
AS SELECT B, SUM(D) FROM T GROUP BY B;
CREATE MATERIALIZED VIEW v3
BUILD IMMEDIATE
REFRESH COMPLETE ON DEMAND
ENABLE QUERY REWRITE
AS SELECT B, C, SUM(D) FROM T GROUP BY B, C;
CREATE MATERIALIZED VIEW v4
BUILD IMMEDIATE
REFRESH COMPLETE ON DEMAND
ENABLE QUERY REWRITE
AS SELECT SUM(D) FROM T;
"
Bt = 10000
T = 512000
Bt = 10000
Tup = 512000
Bt = 10000
T = 512000
Bt/T
Bt/(T/2)/5
Bt = 10000
T = 512000
n.att = 5
B.att = Bt/(T*n.att)
B.att
Bt = 10000
T = 512000
n.att = 5
B.att = Bt/(T*n.att/2)
B.att
B.att*4*80
B.att*5*80*80
Bt = 10000
T = 512000
n.att = 5
B.att = Bt/(T*n.att)
B.att*4*80
B.att*5*80*80
Bt = 10000
T = 512000
n.att = 5
B.att = Bt/(T*n.att)
B.att*5*80*80
T = 512000
n.att = 5
B.att = Bt/(T*n.att)
80*80*80*B.att*5
cost5 <- c(100, 100, 10000, 100)
costs <- list(cost1, cost2, cost3, cost4, cost5)
for (i in costs){
print(calculate_cost(perc, i))
}
0.75^3
log(4)
log(3)
log(2)
log(1)
log(2)
log(3)
log(4)
log(5)
10/1.60
log(5)*6.25
log(5)
10/1.609438
log(5)*6.21
log(4)*6.21
log(3)*6.21
log(2)*6.21
10 - log(2)*6.21
10 - log(3)*6.21
10 - log(4)*6.21
10 - log(5)*6.21
log(2)*6.21
log(2)*6.21
log(3)*6.21
log(4)*6.21
log(5)*6.21
P = 41/45
R = 41/50
2 * (P*R)/(P+R)
?uci
install.packages("readMLData")
?readMLdata
?readMLData
help(readMLData)
library(readMLData)
help("readMLData-package")
yeast <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"),header = FALSE)
View(yeast)
?read.csv
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"),header = FALSE)
View(yeast)
summary(yeast)
?read.table
uninstall.package("readMLData")
remove.package("readMLData")
remove.packages("readMLData")
?read.table
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/yeast/yeast.data"),header = FALSE, row.names = 1)
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),row.names = 1, header = FALSE)
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),row.names = 1, header = TRUE)
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),header = TRUE)
summary(yeast)
View(yeast)
View(yeast)
row.names(yeast)
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),header = FALSE)
yeast[,1]
yeast[,1]
row.names(yeast) <- yeast[,1]
name <- yeast[,1]
yeast <- yeast[,-1]
row.names(yeast) <- name
# Reading the dataset
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),header = FALSE)
summary(yeast)
summary(yeast[,1])
sum(yeast[,1]>1)
sum(yeast[,1] >= 2)
str(yeast)
row.names(yeast)
col.names(yeast)
row.names(yeast)
col.names(yeast) <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc")
cols.names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc")
col.names(yeast) <- cols.names
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),header = TRUE)
summary(yeast)
yeast <- read.table(url("http://archive.ics.uci.edu/ml/machine-learning-databases/
yeast/yeast.data"),header = FALSE)
summary(yeast)
str(yeast)
row.names(yeast)
cols.names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc")
col.names(yeast) <- cols.names
combn(10000,50)
combn(3,2)
length(combn(3,2))
length(combn(10000,2))
dim(combn(10000,50))
dim(combn(1000,50))
dim(combn(100,50))
dim(combn(70,50))
names(yeast) <- cols.names
summary(yeast)
names(yeast)
dim(yeast)
length(cols.names)
cols.names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc","class")
names(yeast) <- cols.names
summary(yeast)
dim(yeast)
summary(yeast)
str(yeast)
}rowsum(yeast)
rowsum(yeast)
par(mfrow = c(3,3))
for (i in 2:ncol(yeast)) {
hist(yeast[,1])
}
str(yeast[,2])
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i])
}
levels(yeast[,ncol(yeast)])
density(yeast[,ncol(yeast)])
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = TRUE, main = "Hist of variable " + names(yeast)[i])
}
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = TRUE, main = names(yeast)[i])
}
?barplot
barplot(yeast[,ncol(yeast)], main = names(yeast)[ncol(yeast)])
barplot(yeast[,10], main = names(yeast)[ncol(yeast)])
yeast[,10]
barplot(yeast[,10])
barplot(table(yeast[,10]))
barplot(table(yeast[,10]), freq = TRUE)
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = TRUE, main = names(yeast)[i])
}
barplot(table(yeast[,10]))
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], main = names(yeast)[i])
}
barplot(table(yeast[,10]))
?hist
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = FALSE, main = names(yeast)[i])
}
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = FALSE, main = names(yeast)[i])
}
barplot(table(yeast[,10]))
density(table(yeast[,10]))
plot(density(table(yeast[,10])))
barplot(table(yeast[,10]))
table(yeast[,10])
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], freq = FALSE, main = names(yeast)[i])
}
barplot(table(yeast[,10]))
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], main = names(yeast)[i])
}
barplot(table(yeast[,10]))
plot(log(yeast[,6]))
plot(log(1+yeast[,6]))
par(mfrow = c(1,1))
plot(log(1+yeast[,6]))
summary(yeast[,6:7])
par(mfrow = c(3,3))
for (i in 2:(ncol(yeast)-1)) {
hist(yeast[,i], main = names(yeast)[i])
}
barplot(table(yeast[,10]))
x = 1
log(6-x)*6.21
x = 2
log(6-x)*6.21
x = 3
log(6-x)*6.21
x = 4
log(6-x)*6.21
set.seed(777)
# 0. Reading the dataset
# Locally
setwd("/Users/manel/Documents/Universidad/MIRI/Q1B/ML/my_labs/machine_learning.git/final_project")
yeast <- read.csv2(file = "yeast.csv", header = FALSE, dec = ".")
library("FactoMineR")
par(mfrow = c(1,2))
pca_yeast <- PCA(yeast)
?PCA
cols.names <- c("seq.name", "mcg", "gvh", "alm", "mit", "erl", "pox", "vac", "nuc","class")
names(yeast) <- cols.names
# The first column should be treated as the row.names but there are inputs that are repeated
# To deal with this problem, the function make names is ideal
row.names(yeast) <- make.names(yeast$seq.name, unique = TRUE)
yeast <- yeast[,-1]
# Basic inspection of the dataset
dim(yeast)
summary(yeast)
dim(yeast)
summary(yeast)
str(yeast)
# the erl variable appears as numerical but in fact it is a binary variable, so it
# should be redefined as a factor
yeast$erl <- as.factor(yeast$erl)
# Plots of 2 vs 2 variables
plot(yeast)
summary(yeast)
dim(yeast)
pca_yeast <- PCA(yeast, quali.sup = c(5,9))
par(mfrow = c(1,1))
plot(pca_yeast$ind$coord, col = yeast$class)
plot(pca_yeast$eig$eigenvalue, type = "b", main = "Eigenvalues")
pca_yeast$eig
